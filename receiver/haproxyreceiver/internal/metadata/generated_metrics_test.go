// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"testing"

	"github.com/stretchr/testify/assert"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
	"go.opentelemetry.io/collector/receiver/receivertest"
	"go.uber.org/zap"
	"go.uber.org/zap/zaptest/observer"
)

type testDataSet int

const (
	testDataSetDefault testDataSet = iota
	testDataSetAll
	testDataSetNone
	testDataSetReag
)

func TestMetricsBuilder(t *testing.T) {
	tests := []struct {
		name        string
		metricsSet  testDataSet
		resAttrsSet testDataSet
		expectEmpty bool
	}{
		{
			name: "default",
		},
		{
			name:        "all_set",
			metricsSet:  testDataSetAll,
			resAttrsSet: testDataSetAll,
		},
		{
			name:        "reaggregate_set",
			metricsSet:  testDataSetReag,
			resAttrsSet: testDataSetReag,
		},
		{
			name:        "none_set",
			metricsSet:  testDataSetNone,
			resAttrsSet: testDataSetNone,
			expectEmpty: true,
		},
		{
			name:        "filter_set_include",
			resAttrsSet: testDataSetAll,
		},
		{
			name:        "filter_set_exclude",
			resAttrsSet: testDataSetAll,
			expectEmpty: true,
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			start := pcommon.Timestamp(1_000_000_000)
			ts := pcommon.Timestamp(1_000_001_000)
			observedZapCore, observedLogs := observer.New(zap.WarnLevel)
			settings := receivertest.NewNopSettings(receivertest.NopType)
			settings.Logger = zap.New(observedZapCore)
			mb := NewMetricsBuilder(loadMetricsBuilderConfig(t, tt.name), settings, WithStartTime(start))
			aggMap := make(map[string]string) // contains the aggregation strategies for each metric name
			aggMap["HaproxyActive"] = mb.metricHaproxyActive.config.AggregationStrategy
			aggMap["HaproxyBackup"] = mb.metricHaproxyBackup.config.AggregationStrategy
			aggMap["HaproxyBytesInput"] = mb.metricHaproxyBytesInput.config.AggregationStrategy
			aggMap["HaproxyBytesOutput"] = mb.metricHaproxyBytesOutput.config.AggregationStrategy
			aggMap["HaproxyClientsCanceled"] = mb.metricHaproxyClientsCanceled.config.AggregationStrategy
			aggMap["HaproxyCompressionBypass"] = mb.metricHaproxyCompressionBypass.config.AggregationStrategy
			aggMap["HaproxyCompressionCount"] = mb.metricHaproxyCompressionCount.config.AggregationStrategy
			aggMap["HaproxyCompressionInput"] = mb.metricHaproxyCompressionInput.config.AggregationStrategy
			aggMap["HaproxyCompressionOutput"] = mb.metricHaproxyCompressionOutput.config.AggregationStrategy
			aggMap["HaproxyConnectionsAverageTime"] = mb.metricHaproxyConnectionsAverageTime.config.AggregationStrategy
			aggMap["HaproxyConnectionsErrors"] = mb.metricHaproxyConnectionsErrors.config.AggregationStrategy
			aggMap["HaproxyConnectionsRate"] = mb.metricHaproxyConnectionsRate.config.AggregationStrategy
			aggMap["HaproxyConnectionsRetries"] = mb.metricHaproxyConnectionsRetries.config.AggregationStrategy
			aggMap["HaproxyConnectionsTotal"] = mb.metricHaproxyConnectionsTotal.config.AggregationStrategy
			aggMap["HaproxyDowntime"] = mb.metricHaproxyDowntime.config.AggregationStrategy
			aggMap["HaproxyFailedChecks"] = mb.metricHaproxyFailedChecks.config.AggregationStrategy
			aggMap["HaproxyRequestsAverageTime"] = mb.metricHaproxyRequestsAverageTime.config.AggregationStrategy
			aggMap["HaproxyRequestsDenied"] = mb.metricHaproxyRequestsDenied.config.AggregationStrategy
			aggMap["HaproxyRequestsErrors"] = mb.metricHaproxyRequestsErrors.config.AggregationStrategy
			aggMap["HaproxyRequestsQueued"] = mb.metricHaproxyRequestsQueued.config.AggregationStrategy
			aggMap["HaproxyRequestsRate"] = mb.metricHaproxyRequestsRate.config.AggregationStrategy
			aggMap["HaproxyRequestsRedispatched"] = mb.metricHaproxyRequestsRedispatched.config.AggregationStrategy
			aggMap["HaproxyRequestsTotal"] = mb.metricHaproxyRequestsTotal.config.AggregationStrategy
			aggMap["HaproxyResponsesAverageTime"] = mb.metricHaproxyResponsesAverageTime.config.AggregationStrategy
			aggMap["HaproxyResponsesDenied"] = mb.metricHaproxyResponsesDenied.config.AggregationStrategy
			aggMap["HaproxyResponsesErrors"] = mb.metricHaproxyResponsesErrors.config.AggregationStrategy
			aggMap["HaproxyServerSelectedTotal"] = mb.metricHaproxyServerSelectedTotal.config.AggregationStrategy
			aggMap["HaproxySessionsAverage"] = mb.metricHaproxySessionsAverage.config.AggregationStrategy
			aggMap["HaproxySessionsCount"] = mb.metricHaproxySessionsCount.config.AggregationStrategy
			aggMap["HaproxySessionsLimit"] = mb.metricHaproxySessionsLimit.config.AggregationStrategy
			aggMap["HaproxySessionsRate"] = mb.metricHaproxySessionsRate.config.AggregationStrategy
			aggMap["HaproxySessionsTotal"] = mb.metricHaproxySessionsTotal.config.AggregationStrategy
			aggMap["HaproxyWeight"] = mb.metricHaproxyWeight.config.AggregationStrategy

			expectedWarnings := 0
			if tt.metricsSet != testDataSetReag {
				assert.Equal(t, expectedWarnings, observedLogs.Len())
			}

			defaultMetricsCount := 0
			allMetricsCount := 0

			allMetricsCount++
			mb.RecordHaproxyActiveDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyActiveDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordHaproxyBackupDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyBackupDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordHaproxyBytesInputDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyBytesInputDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordHaproxyBytesOutputDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyBytesOutputDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordHaproxyClientsCanceledDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyClientsCanceledDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordHaproxyCompressionBypassDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyCompressionBypassDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordHaproxyCompressionCountDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyCompressionCountDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordHaproxyCompressionInputDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyCompressionInputDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordHaproxyCompressionOutputDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyCompressionOutputDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordHaproxyConnectionsAverageTimeDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyConnectionsAverageTimeDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordHaproxyConnectionsErrorsDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyConnectionsErrorsDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordHaproxyConnectionsRateDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyConnectionsRateDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordHaproxyConnectionsRetriesDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyConnectionsRetriesDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordHaproxyConnectionsTotalDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyConnectionsTotalDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordHaproxyDowntimeDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyDowntimeDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordHaproxyFailedChecksDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyFailedChecksDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordHaproxyRequestsAverageTimeDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyRequestsAverageTimeDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordHaproxyRequestsDeniedDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyRequestsDeniedDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordHaproxyRequestsErrorsDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyRequestsErrorsDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordHaproxyRequestsQueuedDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyRequestsQueuedDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordHaproxyRequestsRateDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyRequestsRateDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordHaproxyRequestsRedispatchedDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyRequestsRedispatchedDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordHaproxyRequestsTotalDataPoint(ts, "1", AttributeStatusCode1xx)
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyRequestsTotalDataPoint(ts, "3", AttributeStatusCode2xx)
			}

			allMetricsCount++
			mb.RecordHaproxyResponsesAverageTimeDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyResponsesAverageTimeDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordHaproxyResponsesDeniedDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyResponsesDeniedDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordHaproxyResponsesErrorsDataPoint(ts, 1)
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyResponsesErrorsDataPoint(ts, 3)
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordHaproxyServerSelectedTotalDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyServerSelectedTotalDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordHaproxySessionsAverageDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxySessionsAverageDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordHaproxySessionsCountDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxySessionsCountDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordHaproxySessionsLimitDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxySessionsLimitDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordHaproxySessionsRateDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxySessionsRateDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordHaproxySessionsTotalDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxySessionsTotalDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordHaproxyWeightDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordHaproxyWeightDataPoint(ts, "3")
			}

			rb := mb.NewResourceBuilder()
			rb.SetHaproxyAddr("haproxy.addr-val")
			rb.SetHaproxyProxyName("haproxy.proxy_name-val")
			rb.SetHaproxyServiceName("haproxy.service_name-val")
			res := rb.Emit()
			metrics := mb.Emit(WithResource(res))
			if tt.name == "reaggregate_set" {
				assert.Empty(t, mb.metricHaproxyActive.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyBackup.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyBytesInput.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyBytesOutput.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyClientsCanceled.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyCompressionBypass.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyCompressionCount.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyCompressionInput.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyCompressionOutput.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyConnectionsAverageTime.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyConnectionsErrors.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyConnectionsRate.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyConnectionsRetries.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyConnectionsTotal.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyDowntime.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyFailedChecks.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyRequestsAverageTime.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyRequestsDenied.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyRequestsErrors.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyRequestsQueued.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyRequestsRate.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyRequestsRedispatched.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyRequestsTotal.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyResponsesAverageTime.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyResponsesDenied.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyResponsesErrors.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyServerSelectedTotal.aggDataPoints)
				assert.Empty(t, mb.metricHaproxySessionsAverage.aggDataPoints)
				assert.Empty(t, mb.metricHaproxySessionsCount.aggDataPoints)
				assert.Empty(t, mb.metricHaproxySessionsLimit.aggDataPoints)
				assert.Empty(t, mb.metricHaproxySessionsRate.aggDataPoints)
				assert.Empty(t, mb.metricHaproxySessionsTotal.aggDataPoints)
				assert.Empty(t, mb.metricHaproxyWeight.aggDataPoints)
			}

			if tt.expectEmpty {
				assert.Equal(t, 0, metrics.ResourceMetrics().Len())
				return
			}

			assert.Equal(t, 1, metrics.ResourceMetrics().Len())
			rm := metrics.ResourceMetrics().At(0)
			assert.Equal(t, res, rm.Resource())
			assert.Equal(t, 1, rm.ScopeMetrics().Len())
			ms := rm.ScopeMetrics().At(0).Metrics()
			if tt.metricsSet == testDataSetDefault {
				assert.Equal(t, defaultMetricsCount, ms.Len())
			}
			if tt.metricsSet == testDataSetAll {
				assert.Equal(t, allMetricsCount, ms.Len())
			}
			validatedMetrics := make(map[string]bool)
			for i := 0; i < ms.Len(); i++ {
				switch ms.At(i).Name() {
				case "haproxy.active":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.active"], "Found a duplicate in the metrics slice: haproxy.active")
						validatedMetrics["haproxy.active"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Number of active servers (backend) or server is active (server). Corresponds to HAProxy's `act` metric.", ms.At(i).Description())
						assert.Equal(t, "{servers}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.active"], "Found a duplicate in the metrics slice: haproxy.active")
						validatedMetrics["haproxy.active"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Number of active servers (backend) or server is active (server). Corresponds to HAProxy's `act` metric.", ms.At(i).Description())
						assert.Equal(t, "{servers}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.active"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.backup":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.backup"], "Found a duplicate in the metrics slice: haproxy.backup")
						validatedMetrics["haproxy.backup"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Number of backup servers (backend) or server is backup (server). Corresponds to HAProxy's `bck` metric.", ms.At(i).Description())
						assert.Equal(t, "{servers}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.backup"], "Found a duplicate in the metrics slice: haproxy.backup")
						validatedMetrics["haproxy.backup"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Number of backup servers (backend) or server is backup (server). Corresponds to HAProxy's `bck` metric.", ms.At(i).Description())
						assert.Equal(t, "{servers}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.backup"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.bytes.input":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.bytes.input"], "Found a duplicate in the metrics slice: haproxy.bytes.input")
						validatedMetrics["haproxy.bytes.input"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Bytes in. Corresponds to HAProxy's `bin` metric.", ms.At(i).Description())
						assert.Equal(t, "by", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.bytes.input"], "Found a duplicate in the metrics slice: haproxy.bytes.input")
						validatedMetrics["haproxy.bytes.input"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Bytes in. Corresponds to HAProxy's `bin` metric.", ms.At(i).Description())
						assert.Equal(t, "by", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.bytes.input"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.bytes.output":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.bytes.output"], "Found a duplicate in the metrics slice: haproxy.bytes.output")
						validatedMetrics["haproxy.bytes.output"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Bytes out. Corresponds to HAProxy's `bout` metric.", ms.At(i).Description())
						assert.Equal(t, "by", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.bytes.output"], "Found a duplicate in the metrics slice: haproxy.bytes.output")
						validatedMetrics["haproxy.bytes.output"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Bytes out. Corresponds to HAProxy's `bout` metric.", ms.At(i).Description())
						assert.Equal(t, "by", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.bytes.output"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.clients.canceled":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.clients.canceled"], "Found a duplicate in the metrics slice: haproxy.clients.canceled")
						validatedMetrics["haproxy.clients.canceled"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of data transfers aborted by the client. Corresponds to HAProxy's `cli_abrt` metric", ms.At(i).Description())
						assert.Equal(t, "{cancellations}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.clients.canceled"], "Found a duplicate in the metrics slice: haproxy.clients.canceled")
						validatedMetrics["haproxy.clients.canceled"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of data transfers aborted by the client. Corresponds to HAProxy's `cli_abrt` metric", ms.At(i).Description())
						assert.Equal(t, "{cancellations}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.clients.canceled"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.compression.bypass":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.compression.bypass"], "Found a duplicate in the metrics slice: haproxy.compression.bypass")
						validatedMetrics["haproxy.compression.bypass"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of bytes that bypassed the HTTP compressor (CPU/BW limit). Corresponds to HAProxy's `comp_byp` metric.", ms.At(i).Description())
						assert.Equal(t, "by", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.compression.bypass"], "Found a duplicate in the metrics slice: haproxy.compression.bypass")
						validatedMetrics["haproxy.compression.bypass"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of bytes that bypassed the HTTP compressor (CPU/BW limit). Corresponds to HAProxy's `comp_byp` metric.", ms.At(i).Description())
						assert.Equal(t, "by", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.compression.bypass"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.compression.count":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.compression.count"], "Found a duplicate in the metrics slice: haproxy.compression.count")
						validatedMetrics["haproxy.compression.count"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of HTTP responses that were compressed. Corresponds to HAProxy's `comp_rsp` metric.", ms.At(i).Description())
						assert.Equal(t, "{responses}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.compression.count"], "Found a duplicate in the metrics slice: haproxy.compression.count")
						validatedMetrics["haproxy.compression.count"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of HTTP responses that were compressed. Corresponds to HAProxy's `comp_rsp` metric.", ms.At(i).Description())
						assert.Equal(t, "{responses}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.compression.count"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.compression.input":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.compression.input"], "Found a duplicate in the metrics slice: haproxy.compression.input")
						validatedMetrics["haproxy.compression.input"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of HTTP response bytes fed to the compressor. Corresponds to HAProxy's `comp_in` metric.", ms.At(i).Description())
						assert.Equal(t, "by", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.compression.input"], "Found a duplicate in the metrics slice: haproxy.compression.input")
						validatedMetrics["haproxy.compression.input"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of HTTP response bytes fed to the compressor. Corresponds to HAProxy's `comp_in` metric.", ms.At(i).Description())
						assert.Equal(t, "by", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.compression.input"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.compression.output":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.compression.output"], "Found a duplicate in the metrics slice: haproxy.compression.output")
						validatedMetrics["haproxy.compression.output"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of HTTP response bytes emitted by the compressor. Corresponds to HAProxy's `comp_out` metric.", ms.At(i).Description())
						assert.Equal(t, "by", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.compression.output"], "Found a duplicate in the metrics slice: haproxy.compression.output")
						validatedMetrics["haproxy.compression.output"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of HTTP response bytes emitted by the compressor. Corresponds to HAProxy's `comp_out` metric.", ms.At(i).Description())
						assert.Equal(t, "by", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.compression.output"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.connections.average_time":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.connections.average_time"], "Found a duplicate in the metrics slice: haproxy.connections.average_time")
						validatedMetrics["haproxy.connections.average_time"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Average connect time in ms over the 1024 last requests. Corresponds to HAProxy's `ctime` metric.", ms.At(i).Description())
						assert.Equal(t, "ms", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
						assert.InDelta(t, float64(1), dp.DoubleValue(), 0.01)
					} else {
						assert.False(t, validatedMetrics["haproxy.connections.average_time"], "Found a duplicate in the metrics slice: haproxy.connections.average_time")
						validatedMetrics["haproxy.connections.average_time"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Average connect time in ms over the 1024 last requests. Corresponds to HAProxy's `ctime` metric.", ms.At(i).Description())
						assert.Equal(t, "ms", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
						switch aggMap["haproxy.connections.average_time"] {
						case "sum":
							assert.InDelta(t, float64(4), dp.DoubleValue(), 0.01)
						case "avg":
							assert.InDelta(t, float64(2), dp.DoubleValue(), 0.01)
						case "min":
							assert.InDelta(t, float64(1), dp.DoubleValue(), 0.01)
						case "max":
							assert.InDelta(t, float64(3), dp.DoubleValue(), 0.01)
						}
					}
				case "haproxy.connections.errors":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.connections.errors"], "Found a duplicate in the metrics slice: haproxy.connections.errors")
						validatedMetrics["haproxy.connections.errors"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of requests that encountered an error trying to connect to a backend server. The backend stat is the sum of the stat. Corresponds to HAProxy's `econ` metric", ms.At(i).Description())
						assert.Equal(t, "{errors}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.connections.errors"], "Found a duplicate in the metrics slice: haproxy.connections.errors")
						validatedMetrics["haproxy.connections.errors"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of requests that encountered an error trying to connect to a backend server. The backend stat is the sum of the stat. Corresponds to HAProxy's `econ` metric", ms.At(i).Description())
						assert.Equal(t, "{errors}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.connections.errors"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.connections.rate":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.connections.rate"], "Found a duplicate in the metrics slice: haproxy.connections.rate")
						validatedMetrics["haproxy.connections.rate"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Number of connections over the last elapsed second (frontend). Corresponds to HAProxy's `conn_rate` metric.", ms.At(i).Description())
						assert.Equal(t, "{connections}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.connections.rate"], "Found a duplicate in the metrics slice: haproxy.connections.rate")
						validatedMetrics["haproxy.connections.rate"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Number of connections over the last elapsed second (frontend). Corresponds to HAProxy's `conn_rate` metric.", ms.At(i).Description())
						assert.Equal(t, "{connections}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.connections.rate"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.connections.retries":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.connections.retries"], "Found a duplicate in the metrics slice: haproxy.connections.retries")
						validatedMetrics["haproxy.connections.retries"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times a connection to a server was retried. Corresponds to HAProxy's `wretr` metric.", ms.At(i).Description())
						assert.Equal(t, "{retries}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.connections.retries"], "Found a duplicate in the metrics slice: haproxy.connections.retries")
						validatedMetrics["haproxy.connections.retries"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times a connection to a server was retried. Corresponds to HAProxy's `wretr` metric.", ms.At(i).Description())
						assert.Equal(t, "{retries}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.connections.retries"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.connections.total":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.connections.total"], "Found a duplicate in the metrics slice: haproxy.connections.total")
						validatedMetrics["haproxy.connections.total"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Cumulative number of connections (frontend). Corresponds to HAProxy's `conn_tot` metric.", ms.At(i).Description())
						assert.Equal(t, "{connections}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.connections.total"], "Found a duplicate in the metrics slice: haproxy.connections.total")
						validatedMetrics["haproxy.connections.total"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Cumulative number of connections (frontend). Corresponds to HAProxy's `conn_tot` metric.", ms.At(i).Description())
						assert.Equal(t, "{connections}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.connections.total"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.downtime":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.downtime"], "Found a duplicate in the metrics slice: haproxy.downtime")
						validatedMetrics["haproxy.downtime"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Total downtime (in seconds). The value for the backend is the downtime for the whole backend, not the sum of the server downtime. Corresponds to HAProxy's `downtime` metric", ms.At(i).Description())
						assert.Equal(t, "s", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.downtime"], "Found a duplicate in the metrics slice: haproxy.downtime")
						validatedMetrics["haproxy.downtime"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Total downtime (in seconds). The value for the backend is the downtime for the whole backend, not the sum of the server downtime. Corresponds to HAProxy's `downtime` metric", ms.At(i).Description())
						assert.Equal(t, "s", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.downtime"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.failed_checks":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.failed_checks"], "Found a duplicate in the metrics slice: haproxy.failed_checks")
						validatedMetrics["haproxy.failed_checks"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of failed checks. (Only counts checks failed when the server is up). Corresponds to HAProxy's `chkfail` metric.", ms.At(i).Description())
						assert.Equal(t, "{checks}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.failed_checks"], "Found a duplicate in the metrics slice: haproxy.failed_checks")
						validatedMetrics["haproxy.failed_checks"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of failed checks. (Only counts checks failed when the server is up). Corresponds to HAProxy's `chkfail` metric.", ms.At(i).Description())
						assert.Equal(t, "{checks}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.failed_checks"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.requests.average_time":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.requests.average_time"], "Found a duplicate in the metrics slice: haproxy.requests.average_time")
						validatedMetrics["haproxy.requests.average_time"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Average queue time in ms over the 1024 last requests. Corresponds to HAProxy's `qtime` metric.", ms.At(i).Description())
						assert.Equal(t, "ms", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
						assert.InDelta(t, float64(1), dp.DoubleValue(), 0.01)
					} else {
						assert.False(t, validatedMetrics["haproxy.requests.average_time"], "Found a duplicate in the metrics slice: haproxy.requests.average_time")
						validatedMetrics["haproxy.requests.average_time"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Average queue time in ms over the 1024 last requests. Corresponds to HAProxy's `qtime` metric.", ms.At(i).Description())
						assert.Equal(t, "ms", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
						switch aggMap["haproxy.requests.average_time"] {
						case "sum":
							assert.InDelta(t, float64(4), dp.DoubleValue(), 0.01)
						case "avg":
							assert.InDelta(t, float64(2), dp.DoubleValue(), 0.01)
						case "min":
							assert.InDelta(t, float64(1), dp.DoubleValue(), 0.01)
						case "max":
							assert.InDelta(t, float64(3), dp.DoubleValue(), 0.01)
						}
					}
				case "haproxy.requests.denied":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.requests.denied"], "Found a duplicate in the metrics slice: haproxy.requests.denied")
						validatedMetrics["haproxy.requests.denied"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Requests denied because of security concerns. Corresponds to HAProxy's `dreq` metric", ms.At(i).Description())
						assert.Equal(t, "{requests}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.requests.denied"], "Found a duplicate in the metrics slice: haproxy.requests.denied")
						validatedMetrics["haproxy.requests.denied"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Requests denied because of security concerns. Corresponds to HAProxy's `dreq` metric", ms.At(i).Description())
						assert.Equal(t, "{requests}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.requests.denied"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.requests.errors":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.requests.errors"], "Found a duplicate in the metrics slice: haproxy.requests.errors")
						validatedMetrics["haproxy.requests.errors"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Cumulative number of request errors. Corresponds to HAProxy's `ereq` metric.", ms.At(i).Description())
						assert.Equal(t, "{errors}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.requests.errors"], "Found a duplicate in the metrics slice: haproxy.requests.errors")
						validatedMetrics["haproxy.requests.errors"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Cumulative number of request errors. Corresponds to HAProxy's `ereq` metric.", ms.At(i).Description())
						assert.Equal(t, "{errors}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.requests.errors"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.requests.queued":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.requests.queued"], "Found a duplicate in the metrics slice: haproxy.requests.queued")
						validatedMetrics["haproxy.requests.queued"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Current queued requests. For the backend this reports the number queued without a server assigned. Corresponds to HAProxy's `qcur` metric.", ms.At(i).Description())
						assert.Equal(t, "{requests}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.requests.queued"], "Found a duplicate in the metrics slice: haproxy.requests.queued")
						validatedMetrics["haproxy.requests.queued"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Current queued requests. For the backend this reports the number queued without a server assigned. Corresponds to HAProxy's `qcur` metric.", ms.At(i).Description())
						assert.Equal(t, "{requests}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.requests.queued"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.requests.rate":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.requests.rate"], "Found a duplicate in the metrics slice: haproxy.requests.rate")
						validatedMetrics["haproxy.requests.rate"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "HTTP requests per second over last elapsed second. Corresponds to HAProxy's `req_rate` metric.", ms.At(i).Description())
						assert.Equal(t, "{requests}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
						assert.InDelta(t, float64(1), dp.DoubleValue(), 0.01)
					} else {
						assert.False(t, validatedMetrics["haproxy.requests.rate"], "Found a duplicate in the metrics slice: haproxy.requests.rate")
						validatedMetrics["haproxy.requests.rate"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "HTTP requests per second over last elapsed second. Corresponds to HAProxy's `req_rate` metric.", ms.At(i).Description())
						assert.Equal(t, "{requests}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
						switch aggMap["haproxy.requests.rate"] {
						case "sum":
							assert.InDelta(t, float64(4), dp.DoubleValue(), 0.01)
						case "avg":
							assert.InDelta(t, float64(2), dp.DoubleValue(), 0.01)
						case "min":
							assert.InDelta(t, float64(1), dp.DoubleValue(), 0.01)
						case "max":
							assert.InDelta(t, float64(3), dp.DoubleValue(), 0.01)
						}
					}
				case "haproxy.requests.redispatched":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.requests.redispatched"], "Found a duplicate in the metrics slice: haproxy.requests.redispatched")
						validatedMetrics["haproxy.requests.redispatched"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times a request was redispatched to another server. Corresponds to HAProxy's `wredis` metric.", ms.At(i).Description())
						assert.Equal(t, "{requests}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.requests.redispatched"], "Found a duplicate in the metrics slice: haproxy.requests.redispatched")
						validatedMetrics["haproxy.requests.redispatched"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times a request was redispatched to another server. Corresponds to HAProxy's `wredis` metric.", ms.At(i).Description())
						assert.Equal(t, "{requests}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.requests.redispatched"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.requests.total":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.requests.total"], "Found a duplicate in the metrics slice: haproxy.requests.total")
						validatedMetrics["haproxy.requests.total"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Total number of HTTP requests received. Corresponds to HAProxy's `req_tot`, `hrsp_1xx`, `hrsp_2xx`, `hrsp_3xx`, `hrsp_4xx`, `hrsp_5xx` and `hrsp_other` metrics.", ms.At(i).Description())
						assert.Equal(t, "{requests}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
						attrVal, ok := dp.Attributes().Get("status_code")
						assert.True(t, ok)
						assert.Equal(t, "1xx", attrVal.Str())
					} else {
						assert.False(t, validatedMetrics["haproxy.requests.total"], "Found a duplicate in the metrics slice: haproxy.requests.total")
						validatedMetrics["haproxy.requests.total"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Total number of HTTP requests received. Corresponds to HAProxy's `req_tot`, `hrsp_1xx`, `hrsp_2xx`, `hrsp_3xx`, `hrsp_4xx`, `hrsp_5xx` and `hrsp_other` metrics.", ms.At(i).Description())
						assert.Equal(t, "{requests}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.requests.total"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
						_, ok := dp.Attributes().Get("status_code")
						assert.False(t, ok)
					}
				case "haproxy.responses.average_time":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.responses.average_time"], "Found a duplicate in the metrics slice: haproxy.responses.average_time")
						validatedMetrics["haproxy.responses.average_time"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Average response time in ms over the 1024 last requests. Corresponds to HAProxy's `rtime` metric.", ms.At(i).Description())
						assert.Equal(t, "ms", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
						assert.InDelta(t, float64(1), dp.DoubleValue(), 0.01)
					} else {
						assert.False(t, validatedMetrics["haproxy.responses.average_time"], "Found a duplicate in the metrics slice: haproxy.responses.average_time")
						validatedMetrics["haproxy.responses.average_time"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Average response time in ms over the 1024 last requests. Corresponds to HAProxy's `rtime` metric.", ms.At(i).Description())
						assert.Equal(t, "ms", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
						switch aggMap["haproxy.responses.average_time"] {
						case "sum":
							assert.InDelta(t, float64(4), dp.DoubleValue(), 0.01)
						case "avg":
							assert.InDelta(t, float64(2), dp.DoubleValue(), 0.01)
						case "min":
							assert.InDelta(t, float64(1), dp.DoubleValue(), 0.01)
						case "max":
							assert.InDelta(t, float64(3), dp.DoubleValue(), 0.01)
						}
					}
				case "haproxy.responses.denied":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.responses.denied"], "Found a duplicate in the metrics slice: haproxy.responses.denied")
						validatedMetrics["haproxy.responses.denied"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Responses denied because of security concerns. Corresponds to HAProxy's `dresp` metric", ms.At(i).Description())
						assert.Equal(t, "{responses}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.responses.denied"], "Found a duplicate in the metrics slice: haproxy.responses.denied")
						validatedMetrics["haproxy.responses.denied"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Responses denied because of security concerns. Corresponds to HAProxy's `dresp` metric", ms.At(i).Description())
						assert.Equal(t, "{responses}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.responses.denied"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.responses.errors":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.responses.errors"], "Found a duplicate in the metrics slice: haproxy.responses.errors")
						validatedMetrics["haproxy.responses.errors"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Cumulative number of response errors. Corresponds to HAProxy's `eresp` metric, `srv_abrt` will be counted here also.", ms.At(i).Description())
						assert.Equal(t, "{errors}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.responses.errors"], "Found a duplicate in the metrics slice: haproxy.responses.errors")
						validatedMetrics["haproxy.responses.errors"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Cumulative number of response errors. Corresponds to HAProxy's `eresp` metric, `srv_abrt` will be counted here also.", ms.At(i).Description())
						assert.Equal(t, "{errors}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.responses.errors"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.server_selected.total":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.server_selected.total"], "Found a duplicate in the metrics slice: haproxy.server_selected.total")
						validatedMetrics["haproxy.server_selected.total"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times a server was selected, either for new sessions or when re-dispatching. Corresponds to HAProxy's `lbtot` metric.", ms.At(i).Description())
						assert.Equal(t, "{selections}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.server_selected.total"], "Found a duplicate in the metrics slice: haproxy.server_selected.total")
						validatedMetrics["haproxy.server_selected.total"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times a server was selected, either for new sessions or when re-dispatching. Corresponds to HAProxy's `lbtot` metric.", ms.At(i).Description())
						assert.Equal(t, "{selections}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.server_selected.total"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.sessions.average":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.sessions.average"], "Found a duplicate in the metrics slice: haproxy.sessions.average")
						validatedMetrics["haproxy.sessions.average"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Average total session time in ms over the last 1024 requests. Corresponds to HAProxy's `ttime` metric.", ms.At(i).Description())
						assert.Equal(t, "ms", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
						assert.InDelta(t, float64(1), dp.DoubleValue(), 0.01)
					} else {
						assert.False(t, validatedMetrics["haproxy.sessions.average"], "Found a duplicate in the metrics slice: haproxy.sessions.average")
						validatedMetrics["haproxy.sessions.average"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Average total session time in ms over the last 1024 requests. Corresponds to HAProxy's `ttime` metric.", ms.At(i).Description())
						assert.Equal(t, "ms", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
						switch aggMap["haproxy.sessions.average"] {
						case "sum":
							assert.InDelta(t, float64(4), dp.DoubleValue(), 0.01)
						case "avg":
							assert.InDelta(t, float64(2), dp.DoubleValue(), 0.01)
						case "min":
							assert.InDelta(t, float64(1), dp.DoubleValue(), 0.01)
						case "max":
							assert.InDelta(t, float64(3), dp.DoubleValue(), 0.01)
						}
					}
				case "haproxy.sessions.count":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.sessions.count"], "Found a duplicate in the metrics slice: haproxy.sessions.count")
						validatedMetrics["haproxy.sessions.count"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Current sessions. Corresponds to HAProxy's `scur` metric.", ms.At(i).Description())
						assert.Equal(t, "{sessions}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.sessions.count"], "Found a duplicate in the metrics slice: haproxy.sessions.count")
						validatedMetrics["haproxy.sessions.count"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Current sessions. Corresponds to HAProxy's `scur` metric.", ms.At(i).Description())
						assert.Equal(t, "{sessions}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.sessions.count"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.sessions.limit":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.sessions.limit"], "Found a duplicate in the metrics slice: haproxy.sessions.limit")
						validatedMetrics["haproxy.sessions.limit"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Configured session limit. Corresponds to HAProxy's `slim` metric.", ms.At(i).Description())
						assert.Equal(t, "{sessions}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.sessions.limit"], "Found a duplicate in the metrics slice: haproxy.sessions.limit")
						validatedMetrics["haproxy.sessions.limit"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Configured session limit. Corresponds to HAProxy's `slim` metric.", ms.At(i).Description())
						assert.Equal(t, "{sessions}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.sessions.limit"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.sessions.rate":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.sessions.rate"], "Found a duplicate in the metrics slice: haproxy.sessions.rate")
						validatedMetrics["haproxy.sessions.rate"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Number of sessions per second over last elapsed second. Corresponds to HAProxy's `rate` metric.", ms.At(i).Description())
						assert.Equal(t, "{sessions}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
						assert.InDelta(t, float64(1), dp.DoubleValue(), 0.01)
					} else {
						assert.False(t, validatedMetrics["haproxy.sessions.rate"], "Found a duplicate in the metrics slice: haproxy.sessions.rate")
						validatedMetrics["haproxy.sessions.rate"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Number of sessions per second over last elapsed second. Corresponds to HAProxy's `rate` metric.", ms.At(i).Description())
						assert.Equal(t, "{sessions}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
						switch aggMap["haproxy.sessions.rate"] {
						case "sum":
							assert.InDelta(t, float64(4), dp.DoubleValue(), 0.01)
						case "avg":
							assert.InDelta(t, float64(2), dp.DoubleValue(), 0.01)
						case "min":
							assert.InDelta(t, float64(1), dp.DoubleValue(), 0.01)
						case "max":
							assert.InDelta(t, float64(3), dp.DoubleValue(), 0.01)
						}
					}
				case "haproxy.sessions.total":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.sessions.total"], "Found a duplicate in the metrics slice: haproxy.sessions.total")
						validatedMetrics["haproxy.sessions.total"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Cumulative number of sessions. Corresponds to HAProxy's `stot` metric.", ms.At(i).Description())
						assert.Equal(t, "{sessions}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.sessions.total"], "Found a duplicate in the metrics slice: haproxy.sessions.total")
						validatedMetrics["haproxy.sessions.total"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Cumulative number of sessions. Corresponds to HAProxy's `stot` metric.", ms.At(i).Description())
						assert.Equal(t, "{sessions}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.sessions.total"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "haproxy.weight":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["haproxy.weight"], "Found a duplicate in the metrics slice: haproxy.weight")
						validatedMetrics["haproxy.weight"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Total effective weight (backend) or effective weight (server). Corresponds to HAProxy's `weight` metric.", ms.At(i).Description())
						assert.Equal(t, "1", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["haproxy.weight"], "Found a duplicate in the metrics slice: haproxy.weight")
						validatedMetrics["haproxy.weight"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Total effective weight (backend) or effective weight (server). Corresponds to HAProxy's `weight` metric.", ms.At(i).Description())
						assert.Equal(t, "1", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["haproxy.weight"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				}
			}
		})
	}
}
