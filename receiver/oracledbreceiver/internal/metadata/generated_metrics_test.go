// Code generated by mdatagen. DO NOT EDIT.

package metadata

import (
	"testing"

	"github.com/stretchr/testify/assert"
	"go.opentelemetry.io/collector/pdata/pcommon"
	"go.opentelemetry.io/collector/pdata/pmetric"
	"go.opentelemetry.io/collector/receiver/receivertest"
	"go.uber.org/zap"
	"go.uber.org/zap/zaptest/observer"
)

type testDataSet int

const (
	testDataSetDefault testDataSet = iota
	testDataSetAll
	testDataSetNone
	testDataSetReag
)

func TestMetricsBuilder(t *testing.T) {
	tests := []struct {
		name        string
		metricsSet  testDataSet
		resAttrsSet testDataSet
		expectEmpty bool
	}{
		{
			name: "default",
		},
		{
			name:        "all_set",
			metricsSet:  testDataSetAll,
			resAttrsSet: testDataSetAll,
		},
		{
			name:        "reaggregate_set",
			metricsSet:  testDataSetReag,
			resAttrsSet: testDataSetReag,
		},
		{
			name:        "none_set",
			metricsSet:  testDataSetNone,
			resAttrsSet: testDataSetNone,
			expectEmpty: true,
		},
		{
			name:        "filter_set_include",
			resAttrsSet: testDataSetAll,
		},
		{
			name:        "filter_set_exclude",
			resAttrsSet: testDataSetAll,
			expectEmpty: true,
		},
	}
	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			start := pcommon.Timestamp(1_000_000_000)
			ts := pcommon.Timestamp(1_000_001_000)
			observedZapCore, observedLogs := observer.New(zap.WarnLevel)
			settings := receivertest.NewNopSettings(receivertest.NopType)
			settings.Logger = zap.New(observedZapCore)
			mb := NewMetricsBuilder(loadMetricsBuilderConfig(t, tt.name), settings, WithStartTime(start))
			aggMap := make(map[string]string) // contains the aggregation strategies for each metric name
			aggMap["OracledbConsistentGets"] = mb.metricOracledbConsistentGets.config.AggregationStrategy
			aggMap["OracledbCPUTime"] = mb.metricOracledbCPUTime.config.AggregationStrategy
			aggMap["OracledbDbBlockGets"] = mb.metricOracledbDbBlockGets.config.AggregationStrategy
			aggMap["OracledbDdlStatementsParallelized"] = mb.metricOracledbDdlStatementsParallelized.config.AggregationStrategy
			aggMap["OracledbDmlLocksLimit"] = mb.metricOracledbDmlLocksLimit.config.AggregationStrategy
			aggMap["OracledbDmlLocksUsage"] = mb.metricOracledbDmlLocksUsage.config.AggregationStrategy
			aggMap["OracledbDmlStatementsParallelized"] = mb.metricOracledbDmlStatementsParallelized.config.AggregationStrategy
			aggMap["OracledbEnqueueDeadlocks"] = mb.metricOracledbEnqueueDeadlocks.config.AggregationStrategy
			aggMap["OracledbEnqueueLocksLimit"] = mb.metricOracledbEnqueueLocksLimit.config.AggregationStrategy
			aggMap["OracledbEnqueueLocksUsage"] = mb.metricOracledbEnqueueLocksUsage.config.AggregationStrategy
			aggMap["OracledbEnqueueResourcesLimit"] = mb.metricOracledbEnqueueResourcesLimit.config.AggregationStrategy
			aggMap["OracledbEnqueueResourcesUsage"] = mb.metricOracledbEnqueueResourcesUsage.config.AggregationStrategy
			aggMap["OracledbExchangeDeadlocks"] = mb.metricOracledbExchangeDeadlocks.config.AggregationStrategy
			aggMap["OracledbExecutions"] = mb.metricOracledbExecutions.config.AggregationStrategy
			aggMap["OracledbHardParses"] = mb.metricOracledbHardParses.config.AggregationStrategy
			aggMap["OracledbLogicalReads"] = mb.metricOracledbLogicalReads.config.AggregationStrategy
			aggMap["OracledbLogons"] = mb.metricOracledbLogons.config.AggregationStrategy
			aggMap["OracledbParallelOperationsDowngraded1To25Pct"] = mb.metricOracledbParallelOperationsDowngraded1To25Pct.config.AggregationStrategy
			aggMap["OracledbParallelOperationsDowngraded25To50Pct"] = mb.metricOracledbParallelOperationsDowngraded25To50Pct.config.AggregationStrategy
			aggMap["OracledbParallelOperationsDowngraded50To75Pct"] = mb.metricOracledbParallelOperationsDowngraded50To75Pct.config.AggregationStrategy
			aggMap["OracledbParallelOperationsDowngraded75To99Pct"] = mb.metricOracledbParallelOperationsDowngraded75To99Pct.config.AggregationStrategy
			aggMap["OracledbParallelOperationsDowngradedToSerial"] = mb.metricOracledbParallelOperationsDowngradedToSerial.config.AggregationStrategy
			aggMap["OracledbParallelOperationsNotDowngraded"] = mb.metricOracledbParallelOperationsNotDowngraded.config.AggregationStrategy
			aggMap["OracledbParseCalls"] = mb.metricOracledbParseCalls.config.AggregationStrategy
			aggMap["OracledbPgaMemory"] = mb.metricOracledbPgaMemory.config.AggregationStrategy
			aggMap["OracledbPhysicalReadIoRequests"] = mb.metricOracledbPhysicalReadIoRequests.config.AggregationStrategy
			aggMap["OracledbPhysicalReads"] = mb.metricOracledbPhysicalReads.config.AggregationStrategy
			aggMap["OracledbPhysicalReadsDirect"] = mb.metricOracledbPhysicalReadsDirect.config.AggregationStrategy
			aggMap["OracledbPhysicalWriteIoRequests"] = mb.metricOracledbPhysicalWriteIoRequests.config.AggregationStrategy
			aggMap["OracledbPhysicalWrites"] = mb.metricOracledbPhysicalWrites.config.AggregationStrategy
			aggMap["OracledbPhysicalWritesDirect"] = mb.metricOracledbPhysicalWritesDirect.config.AggregationStrategy
			aggMap["OracledbProcessesLimit"] = mb.metricOracledbProcessesLimit.config.AggregationStrategy
			aggMap["OracledbProcessesUsage"] = mb.metricOracledbProcessesUsage.config.AggregationStrategy
			aggMap["OracledbQueriesParallelized"] = mb.metricOracledbQueriesParallelized.config.AggregationStrategy
			aggMap["OracledbSessionsLimit"] = mb.metricOracledbSessionsLimit.config.AggregationStrategy
			aggMap["OracledbSessionsUsage"] = mb.metricOracledbSessionsUsage.config.AggregationStrategy
			aggMap["OracledbTablespaceSizeLimit"] = mb.metricOracledbTablespaceSizeLimit.config.AggregationStrategy
			aggMap["OracledbTablespaceSizeUsage"] = mb.metricOracledbTablespaceSizeUsage.config.AggregationStrategy
			aggMap["OracledbTransactionsLimit"] = mb.metricOracledbTransactionsLimit.config.AggregationStrategy
			aggMap["OracledbTransactionsUsage"] = mb.metricOracledbTransactionsUsage.config.AggregationStrategy
			aggMap["OracledbUserCommits"] = mb.metricOracledbUserCommits.config.AggregationStrategy
			aggMap["OracledbUserRollbacks"] = mb.metricOracledbUserRollbacks.config.AggregationStrategy

			expectedWarnings := 0
			if tt.metricsSet != testDataSetReag {
				assert.Equal(t, expectedWarnings, observedLogs.Len())
			}

			defaultMetricsCount := 0
			allMetricsCount := 0

			allMetricsCount++
			mb.RecordOracledbConsistentGetsDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbConsistentGetsDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbCPUTimeDataPoint(ts, 1)
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbCPUTimeDataPoint(ts, 3)
			}

			allMetricsCount++
			mb.RecordOracledbDbBlockGetsDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbDbBlockGetsDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordOracledbDdlStatementsParallelizedDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbDdlStatementsParallelizedDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbDmlLocksLimitDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbDmlLocksLimitDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbDmlLocksUsageDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbDmlLocksUsageDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordOracledbDmlStatementsParallelizedDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbDmlStatementsParallelizedDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbEnqueueDeadlocksDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbEnqueueDeadlocksDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbEnqueueLocksLimitDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbEnqueueLocksLimitDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbEnqueueLocksUsageDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbEnqueueLocksUsageDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbEnqueueResourcesLimitDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbEnqueueResourcesLimitDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbEnqueueResourcesUsageDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbEnqueueResourcesUsageDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbExchangeDeadlocksDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbExchangeDeadlocksDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbExecutionsDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbExecutionsDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbHardParsesDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbHardParsesDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbLogicalReadsDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbLogicalReadsDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordOracledbLogonsDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbLogonsDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordOracledbParallelOperationsDowngraded1To25PctDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbParallelOperationsDowngraded1To25PctDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordOracledbParallelOperationsDowngraded25To50PctDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbParallelOperationsDowngraded25To50PctDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordOracledbParallelOperationsDowngraded50To75PctDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbParallelOperationsDowngraded50To75PctDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordOracledbParallelOperationsDowngraded75To99PctDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbParallelOperationsDowngraded75To99PctDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordOracledbParallelOperationsDowngradedToSerialDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbParallelOperationsDowngradedToSerialDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordOracledbParallelOperationsNotDowngradedDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbParallelOperationsNotDowngradedDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbParseCallsDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbParseCallsDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbPgaMemoryDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbPgaMemoryDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordOracledbPhysicalReadIoRequestsDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbPhysicalReadIoRequestsDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbPhysicalReadsDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbPhysicalReadsDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordOracledbPhysicalReadsDirectDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbPhysicalReadsDirectDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordOracledbPhysicalWriteIoRequestsDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbPhysicalWriteIoRequestsDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordOracledbPhysicalWritesDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbPhysicalWritesDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordOracledbPhysicalWritesDirectDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbPhysicalWritesDirectDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbProcessesLimitDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbProcessesLimitDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbProcessesUsageDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbProcessesUsageDataPoint(ts, "3")
			}

			allMetricsCount++
			mb.RecordOracledbQueriesParallelizedDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbQueriesParallelizedDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbSessionsLimitDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbSessionsLimitDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbSessionsUsageDataPoint(ts, "1", "session_type-val", "session_status-val")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbSessionsUsageDataPoint(ts, "3", "session_type-val-2", "session_status-val-2")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbTablespaceSizeLimitDataPoint(ts, 1, "tablespace_name-val")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbTablespaceSizeLimitDataPoint(ts, 3, "tablespace_name-val-2")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbTablespaceSizeUsageDataPoint(ts, 1, "tablespace_name-val")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbTablespaceSizeUsageDataPoint(ts, 3, "tablespace_name-val-2")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbTransactionsLimitDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbTransactionsLimitDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbTransactionsUsageDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbTransactionsUsageDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbUserCommitsDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbUserCommitsDataPoint(ts, "3")
			}

			defaultMetricsCount++
			allMetricsCount++
			mb.RecordOracledbUserRollbacksDataPoint(ts, "1")
			if tt.name == "reaggregate_set" {
				mb.RecordOracledbUserRollbacksDataPoint(ts, "3")
			}

			rb := mb.NewResourceBuilder()
			rb.SetHostName("host.name-val")
			rb.SetOracledbInstanceName("oracledb.instance.name-val")
			rb.SetServiceInstanceID("service.instance.id-val")
			res := rb.Emit()
			metrics := mb.Emit(WithResource(res))
			if tt.name == "reaggregate_set" {
				assert.Empty(t, mb.metricOracledbConsistentGets.aggDataPoints)
				assert.Empty(t, mb.metricOracledbCPUTime.aggDataPoints)
				assert.Empty(t, mb.metricOracledbDbBlockGets.aggDataPoints)
				assert.Empty(t, mb.metricOracledbDdlStatementsParallelized.aggDataPoints)
				assert.Empty(t, mb.metricOracledbDmlLocksLimit.aggDataPoints)
				assert.Empty(t, mb.metricOracledbDmlLocksUsage.aggDataPoints)
				assert.Empty(t, mb.metricOracledbDmlStatementsParallelized.aggDataPoints)
				assert.Empty(t, mb.metricOracledbEnqueueDeadlocks.aggDataPoints)
				assert.Empty(t, mb.metricOracledbEnqueueLocksLimit.aggDataPoints)
				assert.Empty(t, mb.metricOracledbEnqueueLocksUsage.aggDataPoints)
				assert.Empty(t, mb.metricOracledbEnqueueResourcesLimit.aggDataPoints)
				assert.Empty(t, mb.metricOracledbEnqueueResourcesUsage.aggDataPoints)
				assert.Empty(t, mb.metricOracledbExchangeDeadlocks.aggDataPoints)
				assert.Empty(t, mb.metricOracledbExecutions.aggDataPoints)
				assert.Empty(t, mb.metricOracledbHardParses.aggDataPoints)
				assert.Empty(t, mb.metricOracledbLogicalReads.aggDataPoints)
				assert.Empty(t, mb.metricOracledbLogons.aggDataPoints)
				assert.Empty(t, mb.metricOracledbParallelOperationsDowngraded1To25Pct.aggDataPoints)
				assert.Empty(t, mb.metricOracledbParallelOperationsDowngraded25To50Pct.aggDataPoints)
				assert.Empty(t, mb.metricOracledbParallelOperationsDowngraded50To75Pct.aggDataPoints)
				assert.Empty(t, mb.metricOracledbParallelOperationsDowngraded75To99Pct.aggDataPoints)
				assert.Empty(t, mb.metricOracledbParallelOperationsDowngradedToSerial.aggDataPoints)
				assert.Empty(t, mb.metricOracledbParallelOperationsNotDowngraded.aggDataPoints)
				assert.Empty(t, mb.metricOracledbParseCalls.aggDataPoints)
				assert.Empty(t, mb.metricOracledbPgaMemory.aggDataPoints)
				assert.Empty(t, mb.metricOracledbPhysicalReadIoRequests.aggDataPoints)
				assert.Empty(t, mb.metricOracledbPhysicalReads.aggDataPoints)
				assert.Empty(t, mb.metricOracledbPhysicalReadsDirect.aggDataPoints)
				assert.Empty(t, mb.metricOracledbPhysicalWriteIoRequests.aggDataPoints)
				assert.Empty(t, mb.metricOracledbPhysicalWrites.aggDataPoints)
				assert.Empty(t, mb.metricOracledbPhysicalWritesDirect.aggDataPoints)
				assert.Empty(t, mb.metricOracledbProcessesLimit.aggDataPoints)
				assert.Empty(t, mb.metricOracledbProcessesUsage.aggDataPoints)
				assert.Empty(t, mb.metricOracledbQueriesParallelized.aggDataPoints)
				assert.Empty(t, mb.metricOracledbSessionsLimit.aggDataPoints)
				assert.Empty(t, mb.metricOracledbSessionsUsage.aggDataPoints)
				assert.Empty(t, mb.metricOracledbTablespaceSizeLimit.aggDataPoints)
				assert.Empty(t, mb.metricOracledbTablespaceSizeUsage.aggDataPoints)
				assert.Empty(t, mb.metricOracledbTransactionsLimit.aggDataPoints)
				assert.Empty(t, mb.metricOracledbTransactionsUsage.aggDataPoints)
				assert.Empty(t, mb.metricOracledbUserCommits.aggDataPoints)
				assert.Empty(t, mb.metricOracledbUserRollbacks.aggDataPoints)
			}

			if tt.expectEmpty {
				assert.Equal(t, 0, metrics.ResourceMetrics().Len())
				return
			}

			assert.Equal(t, 1, metrics.ResourceMetrics().Len())
			rm := metrics.ResourceMetrics().At(0)
			assert.Equal(t, res, rm.Resource())
			assert.Equal(t, 1, rm.ScopeMetrics().Len())
			ms := rm.ScopeMetrics().At(0).Metrics()
			if tt.metricsSet == testDataSetDefault {
				assert.Equal(t, defaultMetricsCount, ms.Len())
			}
			if tt.metricsSet == testDataSetAll {
				assert.Equal(t, allMetricsCount, ms.Len())
			}
			validatedMetrics := make(map[string]bool)
			for i := 0; i < ms.Len(); i++ {
				switch ms.At(i).Name() {
				case "oracledb.consistent_gets":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.consistent_gets"], "Found a duplicate in the metrics slice: oracledb.consistent_gets")
						validatedMetrics["oracledb.consistent_gets"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times a consistent read was requested for a block from the buffer cache.", ms.At(i).Description())
						assert.Equal(t, "{gets}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.consistent_gets"], "Found a duplicate in the metrics slice: oracledb.consistent_gets")
						validatedMetrics["oracledb.consistent_gets"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times a consistent read was requested for a block from the buffer cache.", ms.At(i).Description())
						assert.Equal(t, "{gets}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.consistent_gets"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.cpu_time":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.cpu_time"], "Found a duplicate in the metrics slice: oracledb.cpu_time")
						validatedMetrics["oracledb.cpu_time"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Cumulative CPU time, in seconds", ms.At(i).Description())
						assert.Equal(t, "s", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
						assert.InDelta(t, float64(1), dp.DoubleValue(), 0.01)
					} else {
						assert.False(t, validatedMetrics["oracledb.cpu_time"], "Found a duplicate in the metrics slice: oracledb.cpu_time")
						validatedMetrics["oracledb.cpu_time"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Cumulative CPU time, in seconds", ms.At(i).Description())
						assert.Equal(t, "s", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeDouble, dp.ValueType())
						switch aggMap["oracledb.cpu_time"] {
						case "sum":
							assert.InDelta(t, float64(4), dp.DoubleValue(), 0.01)
						case "avg":
							assert.InDelta(t, float64(2), dp.DoubleValue(), 0.01)
						case "min":
							assert.InDelta(t, float64(1), dp.DoubleValue(), 0.01)
						case "max":
							assert.InDelta(t, float64(3), dp.DoubleValue(), 0.01)
						}
					}
				case "oracledb.db_block_gets":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.db_block_gets"], "Found a duplicate in the metrics slice: oracledb.db_block_gets")
						validatedMetrics["oracledb.db_block_gets"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times a current block was requested from the buffer cache.", ms.At(i).Description())
						assert.Equal(t, "{gets}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.db_block_gets"], "Found a duplicate in the metrics slice: oracledb.db_block_gets")
						validatedMetrics["oracledb.db_block_gets"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times a current block was requested from the buffer cache.", ms.At(i).Description())
						assert.Equal(t, "{gets}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.db_block_gets"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.ddl_statements_parallelized":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.ddl_statements_parallelized"], "Found a duplicate in the metrics slice: oracledb.ddl_statements_parallelized")
						validatedMetrics["oracledb.ddl_statements_parallelized"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of DDL statements that were executed in parallel", ms.At(i).Description())
						assert.Equal(t, "{statements}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.ddl_statements_parallelized"], "Found a duplicate in the metrics slice: oracledb.ddl_statements_parallelized")
						validatedMetrics["oracledb.ddl_statements_parallelized"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of DDL statements that were executed in parallel", ms.At(i).Description())
						assert.Equal(t, "{statements}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.ddl_statements_parallelized"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.dml_locks.limit":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.dml_locks.limit"], "Found a duplicate in the metrics slice: oracledb.dml_locks.limit")
						validatedMetrics["oracledb.dml_locks.limit"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Maximum limit of active DML (Data Manipulation Language) locks, -1 if unlimited.", ms.At(i).Description())
						assert.Equal(t, "{locks}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.dml_locks.limit"], "Found a duplicate in the metrics slice: oracledb.dml_locks.limit")
						validatedMetrics["oracledb.dml_locks.limit"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Maximum limit of active DML (Data Manipulation Language) locks, -1 if unlimited.", ms.At(i).Description())
						assert.Equal(t, "{locks}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.dml_locks.limit"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.dml_locks.usage":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.dml_locks.usage"], "Found a duplicate in the metrics slice: oracledb.dml_locks.usage")
						validatedMetrics["oracledb.dml_locks.usage"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Current count of active DML (Data Manipulation Language) locks.", ms.At(i).Description())
						assert.Equal(t, "{locks}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.dml_locks.usage"], "Found a duplicate in the metrics slice: oracledb.dml_locks.usage")
						validatedMetrics["oracledb.dml_locks.usage"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Current count of active DML (Data Manipulation Language) locks.", ms.At(i).Description())
						assert.Equal(t, "{locks}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.dml_locks.usage"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.dml_statements_parallelized":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.dml_statements_parallelized"], "Found a duplicate in the metrics slice: oracledb.dml_statements_parallelized")
						validatedMetrics["oracledb.dml_statements_parallelized"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of DML statements that were executed in parallel", ms.At(i).Description())
						assert.Equal(t, "{statements}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.dml_statements_parallelized"], "Found a duplicate in the metrics slice: oracledb.dml_statements_parallelized")
						validatedMetrics["oracledb.dml_statements_parallelized"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of DML statements that were executed in parallel", ms.At(i).Description())
						assert.Equal(t, "{statements}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.dml_statements_parallelized"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.enqueue_deadlocks":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.enqueue_deadlocks"], "Found a duplicate in the metrics slice: oracledb.enqueue_deadlocks")
						validatedMetrics["oracledb.enqueue_deadlocks"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Total number of deadlocks between table or row locks in different sessions.", ms.At(i).Description())
						assert.Equal(t, "{deadlocks}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.enqueue_deadlocks"], "Found a duplicate in the metrics slice: oracledb.enqueue_deadlocks")
						validatedMetrics["oracledb.enqueue_deadlocks"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Total number of deadlocks between table or row locks in different sessions.", ms.At(i).Description())
						assert.Equal(t, "{deadlocks}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.enqueue_deadlocks"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.enqueue_locks.limit":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.enqueue_locks.limit"], "Found a duplicate in the metrics slice: oracledb.enqueue_locks.limit")
						validatedMetrics["oracledb.enqueue_locks.limit"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Maximum limit of active enqueue locks, -1 if unlimited.", ms.At(i).Description())
						assert.Equal(t, "{locks}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.enqueue_locks.limit"], "Found a duplicate in the metrics slice: oracledb.enqueue_locks.limit")
						validatedMetrics["oracledb.enqueue_locks.limit"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Maximum limit of active enqueue locks, -1 if unlimited.", ms.At(i).Description())
						assert.Equal(t, "{locks}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.enqueue_locks.limit"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.enqueue_locks.usage":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.enqueue_locks.usage"], "Found a duplicate in the metrics slice: oracledb.enqueue_locks.usage")
						validatedMetrics["oracledb.enqueue_locks.usage"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Current count of active enqueue locks.", ms.At(i).Description())
						assert.Equal(t, "{locks}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.enqueue_locks.usage"], "Found a duplicate in the metrics slice: oracledb.enqueue_locks.usage")
						validatedMetrics["oracledb.enqueue_locks.usage"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Current count of active enqueue locks.", ms.At(i).Description())
						assert.Equal(t, "{locks}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.enqueue_locks.usage"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.enqueue_resources.limit":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.enqueue_resources.limit"], "Found a duplicate in the metrics slice: oracledb.enqueue_resources.limit")
						validatedMetrics["oracledb.enqueue_resources.limit"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Maximum limit of active enqueue resources, -1 if unlimited.", ms.At(i).Description())
						assert.Equal(t, "{resources}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.enqueue_resources.limit"], "Found a duplicate in the metrics slice: oracledb.enqueue_resources.limit")
						validatedMetrics["oracledb.enqueue_resources.limit"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Maximum limit of active enqueue resources, -1 if unlimited.", ms.At(i).Description())
						assert.Equal(t, "{resources}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.enqueue_resources.limit"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.enqueue_resources.usage":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.enqueue_resources.usage"], "Found a duplicate in the metrics slice: oracledb.enqueue_resources.usage")
						validatedMetrics["oracledb.enqueue_resources.usage"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Current count of active enqueue resources.", ms.At(i).Description())
						assert.Equal(t, "{resources}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.enqueue_resources.usage"], "Found a duplicate in the metrics slice: oracledb.enqueue_resources.usage")
						validatedMetrics["oracledb.enqueue_resources.usage"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Current count of active enqueue resources.", ms.At(i).Description())
						assert.Equal(t, "{resources}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.enqueue_resources.usage"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.exchange_deadlocks":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.exchange_deadlocks"], "Found a duplicate in the metrics slice: oracledb.exchange_deadlocks")
						validatedMetrics["oracledb.exchange_deadlocks"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times that a process detected a potential deadlock when exchanging two buffers and raised an internal, restartable error. Index scans are the only operations that perform exchanges.", ms.At(i).Description())
						assert.Equal(t, "{deadlocks}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.exchange_deadlocks"], "Found a duplicate in the metrics slice: oracledb.exchange_deadlocks")
						validatedMetrics["oracledb.exchange_deadlocks"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times that a process detected a potential deadlock when exchanging two buffers and raised an internal, restartable error. Index scans are the only operations that perform exchanges.", ms.At(i).Description())
						assert.Equal(t, "{deadlocks}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.exchange_deadlocks"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.executions":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.executions"], "Found a duplicate in the metrics slice: oracledb.executions")
						validatedMetrics["oracledb.executions"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Total number of calls (user and recursive) that executed SQL statements", ms.At(i).Description())
						assert.Equal(t, "{executions}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.executions"], "Found a duplicate in the metrics slice: oracledb.executions")
						validatedMetrics["oracledb.executions"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Total number of calls (user and recursive) that executed SQL statements", ms.At(i).Description())
						assert.Equal(t, "{executions}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.executions"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.hard_parses":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.hard_parses"], "Found a duplicate in the metrics slice: oracledb.hard_parses")
						validatedMetrics["oracledb.hard_parses"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of hard parses", ms.At(i).Description())
						assert.Equal(t, "{parses}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.hard_parses"], "Found a duplicate in the metrics slice: oracledb.hard_parses")
						validatedMetrics["oracledb.hard_parses"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of hard parses", ms.At(i).Description())
						assert.Equal(t, "{parses}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.hard_parses"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.logical_reads":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.logical_reads"], "Found a duplicate in the metrics slice: oracledb.logical_reads")
						validatedMetrics["oracledb.logical_reads"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of logical reads", ms.At(i).Description())
						assert.Equal(t, "{reads}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.logical_reads"], "Found a duplicate in the metrics slice: oracledb.logical_reads")
						validatedMetrics["oracledb.logical_reads"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of logical reads", ms.At(i).Description())
						assert.Equal(t, "{reads}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.logical_reads"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.logons":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.logons"], "Found a duplicate in the metrics slice: oracledb.logons")
						validatedMetrics["oracledb.logons"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of logon operations", ms.At(i).Description())
						assert.Equal(t, "{operation}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.logons"], "Found a duplicate in the metrics slice: oracledb.logons")
						validatedMetrics["oracledb.logons"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of logon operations", ms.At(i).Description())
						assert.Equal(t, "{operation}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.logons"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.parallel_operations_downgraded_1_to_25_pct":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.parallel_operations_downgraded_1_to_25_pct"], "Found a duplicate in the metrics slice: oracledb.parallel_operations_downgraded_1_to_25_pct")
						validatedMetrics["oracledb.parallel_operations_downgraded_1_to_25_pct"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times parallel execution was requested and the degree of parallelism was reduced down to 1-25% because of insufficient parallel execution servers", ms.At(i).Description())
						assert.Equal(t, "{executions}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.parallel_operations_downgraded_1_to_25_pct"], "Found a duplicate in the metrics slice: oracledb.parallel_operations_downgraded_1_to_25_pct")
						validatedMetrics["oracledb.parallel_operations_downgraded_1_to_25_pct"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times parallel execution was requested and the degree of parallelism was reduced down to 1-25% because of insufficient parallel execution servers", ms.At(i).Description())
						assert.Equal(t, "{executions}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.parallel_operations_downgraded_1_to_25_pct"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.parallel_operations_downgraded_25_to_50_pct":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.parallel_operations_downgraded_25_to_50_pct"], "Found a duplicate in the metrics slice: oracledb.parallel_operations_downgraded_25_to_50_pct")
						validatedMetrics["oracledb.parallel_operations_downgraded_25_to_50_pct"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times parallel execution was requested and the degree of parallelism was reduced down to 25-50% because of insufficient parallel execution servers", ms.At(i).Description())
						assert.Equal(t, "{executions}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.parallel_operations_downgraded_25_to_50_pct"], "Found a duplicate in the metrics slice: oracledb.parallel_operations_downgraded_25_to_50_pct")
						validatedMetrics["oracledb.parallel_operations_downgraded_25_to_50_pct"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times parallel execution was requested and the degree of parallelism was reduced down to 25-50% because of insufficient parallel execution servers", ms.At(i).Description())
						assert.Equal(t, "{executions}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.parallel_operations_downgraded_25_to_50_pct"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.parallel_operations_downgraded_50_to_75_pct":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.parallel_operations_downgraded_50_to_75_pct"], "Found a duplicate in the metrics slice: oracledb.parallel_operations_downgraded_50_to_75_pct")
						validatedMetrics["oracledb.parallel_operations_downgraded_50_to_75_pct"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times parallel execution was requested and the degree of parallelism was reduced down to 50-75% because of insufficient parallel execution servers", ms.At(i).Description())
						assert.Equal(t, "{executions}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.parallel_operations_downgraded_50_to_75_pct"], "Found a duplicate in the metrics slice: oracledb.parallel_operations_downgraded_50_to_75_pct")
						validatedMetrics["oracledb.parallel_operations_downgraded_50_to_75_pct"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times parallel execution was requested and the degree of parallelism was reduced down to 50-75% because of insufficient parallel execution servers", ms.At(i).Description())
						assert.Equal(t, "{executions}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.parallel_operations_downgraded_50_to_75_pct"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.parallel_operations_downgraded_75_to_99_pct":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.parallel_operations_downgraded_75_to_99_pct"], "Found a duplicate in the metrics slice: oracledb.parallel_operations_downgraded_75_to_99_pct")
						validatedMetrics["oracledb.parallel_operations_downgraded_75_to_99_pct"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times parallel execution was requested and the degree of parallelism was reduced down to 75-99% because of insufficient parallel execution servers", ms.At(i).Description())
						assert.Equal(t, "{executions}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.parallel_operations_downgraded_75_to_99_pct"], "Found a duplicate in the metrics slice: oracledb.parallel_operations_downgraded_75_to_99_pct")
						validatedMetrics["oracledb.parallel_operations_downgraded_75_to_99_pct"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times parallel execution was requested and the degree of parallelism was reduced down to 75-99% because of insufficient parallel execution servers", ms.At(i).Description())
						assert.Equal(t, "{executions}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.parallel_operations_downgraded_75_to_99_pct"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.parallel_operations_downgraded_to_serial":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.parallel_operations_downgraded_to_serial"], "Found a duplicate in the metrics slice: oracledb.parallel_operations_downgraded_to_serial")
						validatedMetrics["oracledb.parallel_operations_downgraded_to_serial"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times parallel execution was requested but execution was serial because of insufficient parallel execution servers", ms.At(i).Description())
						assert.Equal(t, "{executions}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.parallel_operations_downgraded_to_serial"], "Found a duplicate in the metrics slice: oracledb.parallel_operations_downgraded_to_serial")
						validatedMetrics["oracledb.parallel_operations_downgraded_to_serial"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times parallel execution was requested but execution was serial because of insufficient parallel execution servers", ms.At(i).Description())
						assert.Equal(t, "{executions}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.parallel_operations_downgraded_to_serial"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.parallel_operations_not_downgraded":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.parallel_operations_not_downgraded"], "Found a duplicate in the metrics slice: oracledb.parallel_operations_not_downgraded")
						validatedMetrics["oracledb.parallel_operations_not_downgraded"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times parallel execution was executed at the requested degree of parallelism", ms.At(i).Description())
						assert.Equal(t, "{executions}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.parallel_operations_not_downgraded"], "Found a duplicate in the metrics slice: oracledb.parallel_operations_not_downgraded")
						validatedMetrics["oracledb.parallel_operations_not_downgraded"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times parallel execution was executed at the requested degree of parallelism", ms.At(i).Description())
						assert.Equal(t, "{executions}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.parallel_operations_not_downgraded"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.parse_calls":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.parse_calls"], "Found a duplicate in the metrics slice: oracledb.parse_calls")
						validatedMetrics["oracledb.parse_calls"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Total number of parse calls.", ms.At(i).Description())
						assert.Equal(t, "{parses}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.parse_calls"], "Found a duplicate in the metrics slice: oracledb.parse_calls")
						validatedMetrics["oracledb.parse_calls"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Total number of parse calls.", ms.At(i).Description())
						assert.Equal(t, "{parses}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.parse_calls"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.pga_memory":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.pga_memory"], "Found a duplicate in the metrics slice: oracledb.pga_memory")
						validatedMetrics["oracledb.pga_memory"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Session PGA (Program Global Area) memory", ms.At(i).Description())
						assert.Equal(t, "By", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.pga_memory"], "Found a duplicate in the metrics slice: oracledb.pga_memory")
						validatedMetrics["oracledb.pga_memory"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Session PGA (Program Global Area) memory", ms.At(i).Description())
						assert.Equal(t, "By", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.pga_memory"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.physical_read_io_requests":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.physical_read_io_requests"], "Found a duplicate in the metrics slice: oracledb.physical_read_io_requests")
						validatedMetrics["oracledb.physical_read_io_requests"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of read requests for application activity", ms.At(i).Description())
						assert.Equal(t, "{requests}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.physical_read_io_requests"], "Found a duplicate in the metrics slice: oracledb.physical_read_io_requests")
						validatedMetrics["oracledb.physical_read_io_requests"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of read requests for application activity", ms.At(i).Description())
						assert.Equal(t, "{requests}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.physical_read_io_requests"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.physical_reads":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.physical_reads"], "Found a duplicate in the metrics slice: oracledb.physical_reads")
						validatedMetrics["oracledb.physical_reads"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of physical reads", ms.At(i).Description())
						assert.Equal(t, "{reads}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.physical_reads"], "Found a duplicate in the metrics slice: oracledb.physical_reads")
						validatedMetrics["oracledb.physical_reads"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of physical reads", ms.At(i).Description())
						assert.Equal(t, "{reads}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.physical_reads"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.physical_reads_direct":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.physical_reads_direct"], "Found a duplicate in the metrics slice: oracledb.physical_reads_direct")
						validatedMetrics["oracledb.physical_reads_direct"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of reads directly from disk, bypassing the buffer cache", ms.At(i).Description())
						assert.Equal(t, "{reads}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.physical_reads_direct"], "Found a duplicate in the metrics slice: oracledb.physical_reads_direct")
						validatedMetrics["oracledb.physical_reads_direct"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of reads directly from disk, bypassing the buffer cache", ms.At(i).Description())
						assert.Equal(t, "{reads}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.physical_reads_direct"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.physical_write_io_requests":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.physical_write_io_requests"], "Found a duplicate in the metrics slice: oracledb.physical_write_io_requests")
						validatedMetrics["oracledb.physical_write_io_requests"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of write requests for application activity", ms.At(i).Description())
						assert.Equal(t, "{requests}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.physical_write_io_requests"], "Found a duplicate in the metrics slice: oracledb.physical_write_io_requests")
						validatedMetrics["oracledb.physical_write_io_requests"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of write requests for application activity", ms.At(i).Description())
						assert.Equal(t, "{requests}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.physical_write_io_requests"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.physical_writes":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.physical_writes"], "Found a duplicate in the metrics slice: oracledb.physical_writes")
						validatedMetrics["oracledb.physical_writes"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of physical writes", ms.At(i).Description())
						assert.Equal(t, "{writes}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.physical_writes"], "Found a duplicate in the metrics slice: oracledb.physical_writes")
						validatedMetrics["oracledb.physical_writes"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of physical writes", ms.At(i).Description())
						assert.Equal(t, "{writes}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.physical_writes"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.physical_writes_direct":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.physical_writes_direct"], "Found a duplicate in the metrics slice: oracledb.physical_writes_direct")
						validatedMetrics["oracledb.physical_writes_direct"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of writes directly to disk, bypassing the buffer cache", ms.At(i).Description())
						assert.Equal(t, "{writes}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.physical_writes_direct"], "Found a duplicate in the metrics slice: oracledb.physical_writes_direct")
						validatedMetrics["oracledb.physical_writes_direct"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of writes directly to disk, bypassing the buffer cache", ms.At(i).Description())
						assert.Equal(t, "{writes}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.physical_writes_direct"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.processes.limit":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.processes.limit"], "Found a duplicate in the metrics slice: oracledb.processes.limit")
						validatedMetrics["oracledb.processes.limit"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Maximum limit of active processes, -1 if unlimited.", ms.At(i).Description())
						assert.Equal(t, "{processes}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.processes.limit"], "Found a duplicate in the metrics slice: oracledb.processes.limit")
						validatedMetrics["oracledb.processes.limit"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Maximum limit of active processes, -1 if unlimited.", ms.At(i).Description())
						assert.Equal(t, "{processes}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.processes.limit"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.processes.usage":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.processes.usage"], "Found a duplicate in the metrics slice: oracledb.processes.usage")
						validatedMetrics["oracledb.processes.usage"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Current count of active processes.", ms.At(i).Description())
						assert.Equal(t, "{processes}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.processes.usage"], "Found a duplicate in the metrics slice: oracledb.processes.usage")
						validatedMetrics["oracledb.processes.usage"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Current count of active processes.", ms.At(i).Description())
						assert.Equal(t, "{processes}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.processes.usage"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.queries_parallelized":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.queries_parallelized"], "Found a duplicate in the metrics slice: oracledb.queries_parallelized")
						validatedMetrics["oracledb.queries_parallelized"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of SELECT statements executed in parallel", ms.At(i).Description())
						assert.Equal(t, "{queries}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.queries_parallelized"], "Found a duplicate in the metrics slice: oracledb.queries_parallelized")
						validatedMetrics["oracledb.queries_parallelized"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of SELECT statements executed in parallel", ms.At(i).Description())
						assert.Equal(t, "{queries}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.queries_parallelized"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.sessions.limit":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.sessions.limit"], "Found a duplicate in the metrics slice: oracledb.sessions.limit")
						validatedMetrics["oracledb.sessions.limit"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Maximum limit of active sessions, -1 if unlimited.", ms.At(i).Description())
						assert.Equal(t, "{sessions}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.sessions.limit"], "Found a duplicate in the metrics slice: oracledb.sessions.limit")
						validatedMetrics["oracledb.sessions.limit"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Maximum limit of active sessions, -1 if unlimited.", ms.At(i).Description())
						assert.Equal(t, "{sessions}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.sessions.limit"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.sessions.usage":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.sessions.usage"], "Found a duplicate in the metrics slice: oracledb.sessions.usage")
						validatedMetrics["oracledb.sessions.usage"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Count of active sessions.", ms.At(i).Description())
						assert.Equal(t, "{sessions}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
						attrVal, ok := dp.Attributes().Get("session_type")
						assert.True(t, ok)
						assert.Equal(t, "session_type-val", attrVal.Str())
						attrVal, ok = dp.Attributes().Get("session_status")
						assert.True(t, ok)
						assert.Equal(t, "session_status-val", attrVal.Str())
					} else {
						assert.False(t, validatedMetrics["oracledb.sessions.usage"], "Found a duplicate in the metrics slice: oracledb.sessions.usage")
						validatedMetrics["oracledb.sessions.usage"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Count of active sessions.", ms.At(i).Description())
						assert.Equal(t, "{sessions}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.sessions.usage"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
						_, ok := dp.Attributes().Get("session_type")
						assert.False(t, ok)
						_, ok = dp.Attributes().Get("session_status")
						assert.False(t, ok)
					}
				case "oracledb.tablespace_size.limit":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.tablespace_size.limit"], "Found a duplicate in the metrics slice: oracledb.tablespace_size.limit")
						validatedMetrics["oracledb.tablespace_size.limit"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Maximum size of tablespace in bytes, -1 if unlimited.", ms.At(i).Description())
						assert.Equal(t, "By", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
						attrVal, ok := dp.Attributes().Get("tablespace_name")
						assert.True(t, ok)
						assert.Equal(t, "tablespace_name-val", attrVal.Str())
					} else {
						assert.False(t, validatedMetrics["oracledb.tablespace_size.limit"], "Found a duplicate in the metrics slice: oracledb.tablespace_size.limit")
						validatedMetrics["oracledb.tablespace_size.limit"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Maximum size of tablespace in bytes, -1 if unlimited.", ms.At(i).Description())
						assert.Equal(t, "By", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.tablespace_size.limit"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
						_, ok := dp.Attributes().Get("tablespace_name")
						assert.False(t, ok)
					}
				case "oracledb.tablespace_size.usage":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.tablespace_size.usage"], "Found a duplicate in the metrics slice: oracledb.tablespace_size.usage")
						validatedMetrics["oracledb.tablespace_size.usage"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Used tablespace in bytes.", ms.At(i).Description())
						assert.Equal(t, "By", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
						attrVal, ok := dp.Attributes().Get("tablespace_name")
						assert.True(t, ok)
						assert.Equal(t, "tablespace_name-val", attrVal.Str())
					} else {
						assert.False(t, validatedMetrics["oracledb.tablespace_size.usage"], "Found a duplicate in the metrics slice: oracledb.tablespace_size.usage")
						validatedMetrics["oracledb.tablespace_size.usage"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Used tablespace in bytes.", ms.At(i).Description())
						assert.Equal(t, "By", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.tablespace_size.usage"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
						_, ok := dp.Attributes().Get("tablespace_name")
						assert.False(t, ok)
					}
				case "oracledb.transactions.limit":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.transactions.limit"], "Found a duplicate in the metrics slice: oracledb.transactions.limit")
						validatedMetrics["oracledb.transactions.limit"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Maximum limit of active transactions, -1 if unlimited.", ms.At(i).Description())
						assert.Equal(t, "{transactions}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.transactions.limit"], "Found a duplicate in the metrics slice: oracledb.transactions.limit")
						validatedMetrics["oracledb.transactions.limit"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Maximum limit of active transactions, -1 if unlimited.", ms.At(i).Description())
						assert.Equal(t, "{transactions}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.transactions.limit"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.transactions.usage":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.transactions.usage"], "Found a duplicate in the metrics slice: oracledb.transactions.usage")
						validatedMetrics["oracledb.transactions.usage"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Current count of active transactions.", ms.At(i).Description())
						assert.Equal(t, "{transactions}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.transactions.usage"], "Found a duplicate in the metrics slice: oracledb.transactions.usage")
						validatedMetrics["oracledb.transactions.usage"] = true
						assert.Equal(t, pmetric.MetricTypeGauge, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Gauge().DataPoints().Len())
						assert.Equal(t, "Current count of active transactions.", ms.At(i).Description())
						assert.Equal(t, "{transactions}", ms.At(i).Unit())
						dp := ms.At(i).Gauge().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.transactions.usage"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.user_commits":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.user_commits"], "Found a duplicate in the metrics slice: oracledb.user_commits")
						validatedMetrics["oracledb.user_commits"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of user commits. When a user commits a transaction, the redo generated that reflects the changes made to database blocks must be written to disk. Commits often represent the closest thing to a user transaction rate.", ms.At(i).Description())
						assert.Equal(t, "{commits}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.user_commits"], "Found a duplicate in the metrics slice: oracledb.user_commits")
						validatedMetrics["oracledb.user_commits"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of user commits. When a user commits a transaction, the redo generated that reflects the changes made to database blocks must be written to disk. Commits often represent the closest thing to a user transaction rate.", ms.At(i).Description())
						assert.Equal(t, "{commits}", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.user_commits"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				case "oracledb.user_rollbacks":
					if tt.name != "reaggregate_set" {
						assert.False(t, validatedMetrics["oracledb.user_rollbacks"], "Found a duplicate in the metrics slice: oracledb.user_rollbacks")
						validatedMetrics["oracledb.user_rollbacks"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times users manually issue the ROLLBACK statement or an error occurs during a user's transactions", ms.At(i).Description())
						assert.Equal(t, "1", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						assert.Equal(t, int64(1), dp.IntValue())
					} else {
						assert.False(t, validatedMetrics["oracledb.user_rollbacks"], "Found a duplicate in the metrics slice: oracledb.user_rollbacks")
						validatedMetrics["oracledb.user_rollbacks"] = true
						assert.Equal(t, pmetric.MetricTypeSum, ms.At(i).Type())
						assert.Equal(t, 1, ms.At(i).Sum().DataPoints().Len())
						assert.Equal(t, "Number of times users manually issue the ROLLBACK statement or an error occurs during a user's transactions", ms.At(i).Description())
						assert.Equal(t, "1", ms.At(i).Unit())
						assert.True(t, ms.At(i).Sum().IsMonotonic())
						assert.Equal(t, pmetric.AggregationTemporalityCumulative, ms.At(i).Sum().AggregationTemporality())
						dp := ms.At(i).Sum().DataPoints().At(0)
						assert.Equal(t, start, dp.StartTimestamp())
						assert.Equal(t, ts, dp.Timestamp())
						assert.Equal(t, pmetric.NumberDataPointValueTypeInt, dp.ValueType())
						switch aggMap["oracledb.user_rollbacks"] {
						case "sum":
							assert.Equal(t, int64(4), dp.IntValue())
						case "avg":
							assert.Equal(t, int64(2), dp.IntValue())
						case "min":
							assert.Equal(t, int64(1), dp.IntValue())
						case "max":
							assert.Equal(t, int64(3), dp.IntValue())
						}
					}
				}
			}
		})
	}
}
